{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import re\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, TrainingArguments, AutoModelForSequenceClassification, Trainer\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_product_dataset = load_dataset(\"json\", data_files=\"amz_products_small.jsonl\", split=\"train\")\n",
    "product_dataset = all_product_dataset.shuffle(seed=42).select(range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'also_buy': [], 'also_view': [], 'asin': 'B00DBHAE5A', 'brand': 'Omer', 'category': ['Automotive', 'Replacement Parts', 'Engines & Engine Parts', 'Engine Parts', 'Engine Mounts'], 'description': ['Now that rock crawling has become the favorite past time for Jeepers, and we are putting our Jeeps in places that they were never designed to go, the stresses on the stock motor mounts are tremendous. With deep gearing, big tires, lockers, and mega torque, the stock mounts can be torn apart in no time at all. That is where these BombProofTM Motor Mounts come into play. These are the ultimate motor mounts that you can install in your Jeep vehicle. They will create a cross- member with the engine, improve clutch linkage operation and beef up your Jeep for serious wheelin. They bolt into stock holes in the frame and engine, and in most applications, no modifications are needed. Black polyurethane bushings insulate from steel to steel contact. All BombProofTM Motor Mounts are bare steel or zinc plated gold.'], 'feature': ['Jeep CJ w/AMC V-8 Engines', 'JM3 Jeep CJs, all, equipped with AMC V 8 engines. These mounts locate the engine in the factory stock location. They allow the use of stock exhaust manifolds, or aftermarket headers (inframe or fender well). Bare steel, no paint.', '9x9x9'], 'image': [], 'price': '$306.66', 'title': 'M.O.R.E. JM301 Bomb Proof Motor Mounts / Polyurethane', 'main_cat': 'Automotive'}\n"
     ]
    }
   ],
   "source": [
    "print(all_product_dataset[50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:01<00:00, 6660.14 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:00<00:00, 20533.04 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:00<00:00, 19461.85 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Remove unnecessary columns\n",
    "columns_to_remove = ['title','feature','brand','also_buy', 'also_view', 'asin', 'category', 'image', 'price']\n",
    "product_dataset = product_dataset.remove_columns(columns_to_remove)\n",
    "\n",
    "# Step 3: Flatten lists in the dataset\n",
    "def flatten_lists(example):\n",
    "    for key, value in example.items():\n",
    "        if isinstance(value, list):\n",
    "            example[key] = ' '.join(value)\n",
    "    return example\n",
    "\n",
    "product_dataset = product_dataset.map(flatten_lists)\n",
    "\n",
    "# Step 4: Lowercase text columns\n",
    "def lowercase_text(example):\n",
    "    text_columns = ['description']\n",
    "    for col in text_columns:\n",
    "        example[col] = example[col].lower()\n",
    "    return example\n",
    "\n",
    "product_dataset = product_dataset.map(lowercase_text)\n",
    "\n",
    "# Step 5: Clean HTML tags from description\n",
    "def clean_html(example):\n",
    "    example['description'] = re.sub(r'<[^>]+>', '', example['description'])\n",
    "    return example\n",
    "\n",
    "# Step 6: Rename main_cat into label\n",
    "product_dataset = product_dataset.rename_column(\n",
    "    original_column_name=\"main_cat\", new_column_name=\"label\"\n",
    ")\n",
    "\n",
    "product_dataset = product_dataset.map(clean_html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:02<00:00, 4781.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the tokenizer\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Step 2: Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"description\"],\n",
    "        padding=\"max_length\",  # Pad sequences to max_length\n",
    "        truncation=True,       # Truncate sequences longer than max_length\n",
    "        max_length=128,        # Set a fixed length for all sequences\n",
    "    )\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_datasets = product_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'lt-840-6a 4ch cv dmx-pwm decoder  input signal: dmx512  input power: dc5vdc24v  max load current: 6a 4ch max 24a  max output power: 120w/288w/576w(5v/12v/24v)  dmx512 socket: xlr-3, rj45, green terminal (with signal amplification function)  dimming range: 0~100%  working temperature: -3065  dimension: l163w78h40mm  package size: l180w82h50mm  weight (g.w): 445g   package include: 1pc * lt-840-6a 4ch cv decoder ', 'label': 'Musical Instruments', 'input_ids': [101, 8318, 1011, 28122, 1011, 1020, 2050, 1018, 2818, 26226, 1040, 22984, 1011, 1052, 2860, 2213, 21933, 4063, 7953, 4742, 1024, 1040, 22984, 22203, 2475, 7953, 2373, 1024, 5887, 2629, 16872, 2278, 18827, 2615, 4098, 7170, 2783, 1024, 1020, 2050, 1018, 2818, 4098, 2484, 2050, 4098, 6434, 2373, 1024, 6036, 2860, 1013, 24841, 2860, 1013, 5401, 2575, 2860, 1006, 1019, 2615, 1013, 2260, 2615, 1013, 2484, 2615, 1007, 1040, 22984, 22203, 2475, 22278, 1024, 28712, 2099, 1011, 1017, 1010, 1054, 3501, 19961, 1010, 2665, 5536, 1006, 2007, 4742, 23713, 3669, 10803, 3853, 1007, 11737, 6562, 2846, 1024, 1014, 1066, 2531, 1003, 2551, 4860, 1024, 1011, 24622, 2629, 9812, 1024, 1048, 16048, 2509, 2860, 2581, 2620, 2232, 12740, 7382, 7427, 2946, 1024, 1048, 15136, 2692, 2860, 2620, 2475, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of the first example's input_ids\n",
    "print(tokenized_datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:02<00:00, 4189.79 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract all main_cat values\n",
    "main_cat_values = product_dataset['label']  # Adjust based on your dataset structure\n",
    "\n",
    "# Fit the label encoder on all main_cat values\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(main_cat_values)\n",
    "\n",
    "# Add encoded labels to the dataset\n",
    "def encode_labels(example):\n",
    "    return {'label': label_encoder.transform([example['label']])[0]}  # Transform single value\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(encode_labels)\n",
    "\n",
    "# Verify the labels\n",
    "print(tokenized_datasets[0]['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: 16\n",
      "Attention mask shape: 16\n"
     ]
    }
   ],
   "source": [
    "# Inspect the first batch of data\n",
    "batch = tokenized_datasets[:16]  # Take a batch of 16 examples\n",
    "print(\"Input IDs shape:\", len(batch[\"input_ids\"]))\n",
    "print(\"Attention mask shape:\", len(batch[\"attention_mask\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print(len(label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\bapti\\OneDrive\\Bureau\\Programmation\\Product Classification\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='596' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 596/1500 1:54:51 < 2:54:47, 0.09 it/s, Epoch 1.19/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.049700</td>\n",
       "      <td>1.223777</td>\n",
       "      <td>0.662000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     31\u001b[39m trainer = Trainer(\n\u001b[32m     32\u001b[39m     model=model,\n\u001b[32m     33\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     data_collator=data_collator,\n\u001b[32m     38\u001b[39m )\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Step 9: Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Step 10: Evaluate the model\u001b[39;00m\n\u001b[32m     44\u001b[39m results = trainer.evaluate()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bapti\\OneDrive\\Bureau\\Programmation\\Product Classification\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2241\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2239\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2240\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bapti\\OneDrive\\Bureau\\Programmation\\Product Classification\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2553\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2547\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m   2548\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   2550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2551\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2552\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m-> \u001b[39m\u001b[32m2553\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2554\u001b[39m ):\n\u001b[32m   2555\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2556\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n\u001b[32m   2557\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Step 5: Load the model\n",
    "num_labels = len(label_encoder.classes_)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n",
    "\n",
    "# Step 6: Set up training argument\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5, \n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Step 7: Define metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "# Step 8: Initialize the Trainer\n",
    "tokenized_datasets = tokenized_datasets.train_test_split(train_size=0.8, seed=42)\n",
    "# Initialize the data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Step 9: Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 10: Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "\n",
    "# Step 11: Save the model\n",
    "model.save_pretrained(\"./final_model\")\n",
    "tokenizer.save_pretrained(\"./final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.223777413368225, 'eval_accuracy': 0.662}\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 2364.05 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\bapti\\OneDrive\\Bureau\\Programmation\\Product Classification\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  6/500 03:29 < 7:10:39, 0.02 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import re\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    ")\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import joblib  # For saving the label encoder\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "all_product_dataset = load_dataset(\"json\", data_files=\"amz_products_small.jsonl\", split=\"train\")\n",
    "product_dataset = all_product_dataset.shuffle(seed=42).select(range(10000))\n",
    "\n",
    "# Step 2: Remove unnecessary columns\n",
    "columns_to_remove = ['title', 'feature', 'brand', 'also_buy', 'also_view', 'asin', 'category', 'image', 'price']\n",
    "product_dataset = product_dataset.remove_columns(columns_to_remove)\n",
    "\n",
    "# Step 3: Flatten lists in the dataset\n",
    "def flatten_lists(example):\n",
    "    for key, value in example.items():\n",
    "        if isinstance(value, list):\n",
    "            example[key] = ' '.join(value)\n",
    "    return example\n",
    "\n",
    "product_dataset = product_dataset.map(flatten_lists)\n",
    "\n",
    "# Step 4: Lowercase text columns\n",
    "def lowercase_text(example):\n",
    "    text_columns = ['description']\n",
    "    for col in text_columns:\n",
    "        example[col] = example[col].lower()\n",
    "    return example\n",
    "\n",
    "product_dataset = product_dataset.map(lowercase_text)\n",
    "\n",
    "# Step 5: Clean HTML tags from description\n",
    "def clean_html(example):\n",
    "    example['description'] = re.sub(r'<[^>]+>', '', example['description'])\n",
    "    return example\n",
    "\n",
    "product_dataset = product_dataset.map(clean_html)\n",
    "\n",
    "# Step 6: Rename main_cat into label\n",
    "product_dataset = product_dataset.rename_column(\n",
    "    original_column_name=\"main_cat\", new_column_name=\"label\"\n",
    ")\n",
    "\n",
    "# Step 7: Split the dataset into train and test sets\n",
    "product_dataset = product_dataset.train_test_split(train_size=0.8, seed=42)\n",
    "\n",
    "# Step 8: Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(product_dataset[\"train\"][\"label\"])  # Fit only on the training set\n",
    "\n",
    "def encode_labels(example):\n",
    "    return {\"label\": label_encoder.transform([example[\"label\"]])[0]}\n",
    "\n",
    "product_dataset = product_dataset.map(encode_labels)\n",
    "\n",
    "# Step 9: Save the label encoder for inference\n",
    "joblib.dump(label_encoder, \"label_encoder.pkl\")\n",
    "\n",
    "# Step 10: Load the tokenizer\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Step 11: Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"description\"],\n",
    "        truncation=True,  # Dynamic padding will be handled by DataCollator\n",
    "    )\n",
    "\n",
    "tokenized_datasets = product_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Step 12: Load the model\n",
    "num_labels = len(label_encoder.classes_)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n",
    "\n",
    "# Step 13: Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,  # Increased for better fine-tuning\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Step 14: Define metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions, average=\"weighted\"),  # Added F1-score\n",
    "    }\n",
    "\n",
    "# Step 15: Initialize the data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# Step 16: Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Step 17: Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 18: Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "\n",
    "# Step 19: Save the model and tokenizer\n",
    "model.save_pretrained(\"./final_model\")\n",
    "tokenizer.save_pretrained(\"./final_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
